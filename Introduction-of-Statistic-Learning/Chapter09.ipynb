{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 09**  \n",
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Maximal Margin Classifier\n",
    "## 1.1 What Is a Hyperplane?\n",
    "In a $p$-dimensional space, a *hyperplane* is a flat affine subspace of dimension $p-1$. \n",
    "\n",
    "In mathematical definition, a $p$-dimensional hyperplane satisfies the following equation:\n",
    "$$\n",
    "\\beta_0+\\beta_1X_1+\\beta_2X_2+\\ldots+\\beta_pX_p=0\n",
    "$$\n",
    "## 1.2 Classification Using a Separating Hyperplane\n",
    "Suppose that we have $n \\times p$ data matrix $X$ that consist of $n$ training observations in $p$-dimensional space, and these observation fall into two classes-that is, $y_1,\\ldots,y_n \\in \\{-1,1\\}$.\n",
    "Then a separating hyperplane has property that\n",
    "$$\n",
    "(\\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2}+\\ldots+\\beta_px_{ip})y_i > 0\n",
    "$$\n",
    "\n",
    "## 1.3 The Maximal Margin Classifier\n",
    "A natural choice is the maximal margin hyperplane, which is the separating hyperplane that is farthest from the training observations.\n",
    "Briefly, the maximal margin hyperplane is the solution to the optimization problem\n",
    "$$\n",
    "\\underset{\\beta_0,\\beta_1,\\ldots,\\beta_p}{\\text{maximize}}M\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "(\\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2}+\\ldots+\\beta_px_{ip})y_i \\ge M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Support Vector Classifier\n",
    "## soft margin classifier\n",
    "It is the solution to the optimization problem\n",
    "$$\n",
    "\\underset{\\beta_0,\\beta_1,\\ldots,\\beta_p}{\\text{maximize}}M\n",
    "$$\n",
    "subject to \n",
    "$$\\sum_{j=1}^{p}\\beta_j^2=1$$\n",
    "$$(\\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2}+\\ldots+\\beta_px_{ip})y_i \\ge M(1-\\epsilon_i)$$\n",
    "$$\\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Support Vector Machines\n",
    "It is an extension of the support vector classifier that results from enlarging the features space in a specific way, using *kernels*. The solution to the suppport vector classifier problem involves only the *inner products* of the observations. The inner product of two $r$-vector $a$ and $b$ is defined as $<a,b>=\\sum_{i=1}^ra_ib_i$. Thus the inner product of two observations $x_i, x_{i'}$ is given by\n",
    "$$\n",
    "<x_i,x_{i'}>=\\sum_{j=1}^{p}x_{ij}x_{x'j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 SVMs with more than two classes\n",
    "## 4.1 One-Versus-One Classification\n",
    "A approach constructs ${k \\choose 2}$ SVMs. We classify a test observation using each of the ${k \\choose 2}$ classifiers, and tally the number of times that test observation is assigned to each of the $K$ classes.\n",
    "## 4.2 One-Versus-All Classification\n",
    "We fit K SVMs, each time comparing one the K classes to the remaining $K-1$ classes. Let $x^{*}$ denote a test observation. We assign the observation to the class for which $\\beta_{0k}+\\beta_{1k}x_1^*+\\ldots+\\beta_{pk}x_p^*$ is largest, as this amounts to a high level of confidence that the test observaton belongs to the $k$th class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
